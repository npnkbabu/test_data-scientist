{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Counting the pairs with k different from an integer list\n",
    "eg: list = [1,3,5] and k = 2\n",
    "expected: we will have 2 pairs: {(1,3), (3,5)}\n",
    "Note: we also consider the negative numbers. \n",
    "\n",
    "sol.<br />\n",
    "Brute force appraoch is to run through 2 for loops to iterate the list and compare each element with rest of the elements. its time complexity is O(n square)\n",
    "efficient approach:<br />\n",
    "Assumption : list is sorted in assending order as per given example. <br />\n",
    "We use sliding window technique with variable window size. compare each adjecent elements and check for difference. Time complexity is O(n) <br/>\n",
    "<br/>\n",
    "if list is not sorted the we need to sort it out first and proceed with below function, which will increase the time complexity to O(nlogn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getDiffPair_Sorted(arr,n): \n",
    "    #taking distinct elements\n",
    "    arr = list(set(arr))\n",
    "    \n",
    "    size = len(arr) \n",
    "    #result set with tuples\n",
    "    res = set()\n",
    "  \n",
    "    # Initialize indices\n",
    "    i,j = 0,1\n",
    "  \n",
    "    # Search pairs sequentially\n",
    "    while i < size and j < size: \n",
    "  \n",
    "        if i != j and arr[j]-arr[i] == n: \n",
    "            res.add((arr[i],arr[j]))\n",
    "            i += 1\n",
    "            j += 1\n",
    "  \n",
    "        elif arr[j] - arr[i] < n: \n",
    "            j+=1\n",
    "        \n",
    "        else: \n",
    "            i+=1\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-5, -7), (-3, -5)}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDiffPair_Sorted([-3,-5,-7,-11,-13] ,-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Return the list of indices. The indices is a sublist points to the same person. The same persons means they have the same name or email or phone. <br />\n",
    "eg: data = [ (\"username1\",\"phone_number1\", \"email1\"), (\"usernameX\",\"phone_number1\", \"emailX\"), (\"usernameZ\",\"phone_numberZ\", \"email1Z\"), (\"usernameY\",\"phone_numberY\", \"emailX\"), ]  <br />\n",
    "expected: [[0,1,3][2]]\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def matchPerson(arr):\n",
    "    data = np.array(arr)\n",
    "    res = []\n",
    "    #compare each element of row with another row and form matrix\n",
    "    matchPersons = sum([user == data for user in data])\n",
    "\n",
    "    # person  match with any other person has sum > 3\n",
    "    match = list(np.where(np.sum(matchPersons, axis=1) > 3)[0])\n",
    "    res.append(match)\n",
    "    \n",
    "    # the person which are not there in match set\n",
    "    nonMatch = list(set(range(len(data))) ^ set(match))\n",
    "    res.append(nonMatch)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 3], [2]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [ ('username1','phone_number1', 'email1'),\n",
    "       ('usernameX','phone_number1', 'emailX'),\n",
    "       ('usernameZ','phone_numberZ', 'email1Z'),\n",
    "       ('usernameY','phone_numberY', 'emailX') ]\n",
    "matchPerson(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Implement the Forward propagation & Backward propagation for a three layers Neural Network. X,W\n",
    "and b can be random. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sol. <br/>\n",
    "Forward Propagation <br/>\n",
    "Forward Propagation refers to the movement of the input through the hidden layers to the output layers. In forward propagation, the information travels in a single direction FORWARD. The input layer supplies the input to the hidden layers and then the output is generated. There is no backward movement.\n",
    "\n",
    "Backpropagation <br/>\n",
    "When we define a neural network, we assign random weights and bias values to our nodes. Once we have received the output for a single iteration, we can calculate the error of the network. This error is then fed back to the network along with the gradient of the cost function to update the weights of the network. These weights are then updated so that the errors in the subsequent iterations are reduced. This updating of weights using the gradient of the cost function is known as back-propagation.\n",
    "<br/>\n",
    "<ol>\n",
    "<li> X : input layer size</li>\n",
    "<li> h : hidden layer size</li>\n",
    "<li> W : weights</li>\n",
    "<li> b : bias</li>\n",
    "<li> Y : output layer size = (_,1)</li>\n",
    "<li> Activation functions : Sigmoid(x) = 1/ (1+e-x), derivative of Sigmoid(x) = Sigmoid(x) * (1 - Sigmoid(x))\n",
    "<li> cost function : means square error = 1/m âˆ‘(y â€“ ^y)2 </li>\n",
    "<li> optimization algo : gradient descent</li>\n",
    "</ol>\n",
    "\n",
    "<ol>\n",
    "    <li> inputs : X,W,b</li>\n",
    "    <li> improvements: we can enhance the below implmentation by using double linked lists for each layer. so that we can add number of hidden layers dynamically. while propagations, we can use liked list concept to move from one layer to next layer in both forward and backward directions</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NN:\n",
    "    def __init__(self, input_layer, hidden_layer, output_layer, W1,W2,B1,B2):\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "        self.input_layer = input_layer\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.B1 = B1\n",
    "        self.B2 = B2\n",
    "        self.error = 0\n",
    "        \n",
    "    def sigmoid(self,X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    #we are using gradient descent optimization, so we take derivative (slope) of activation\n",
    "    def sigmoid_der(self,x):\n",
    "        sig = self.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    def mse(self,y_true, y_pred):\n",
    "        return (np.sum(np.power(y_pred - y_true, 2))) / len(y_true)\n",
    "        \n",
    "    def forwardAndBackwardPro(self,x_train,y_train):\n",
    "        for x, y_true in zip(x_train, y_train):\n",
    "            #forward propagation\n",
    "            #input layer to hidden layer operations\n",
    "            #linear output\n",
    "            h1_ = np.dot(x, self.W1) + self.B1\n",
    "            #non-linear output by sigmoid\n",
    "            h1 = self.sigmoid(h1_)\n",
    "\n",
    "            #hidden layer to output layer operations\n",
    "            #linear output\n",
    "            y_ = np.dot(h1, self.W2) + self.B2\n",
    "            #non-linear output by sigmoid\n",
    "            self.y = self.sigmoid(y_)\n",
    "\n",
    "            # Error computation\n",
    "            self.error += self.mse(y_true,y_train)\n",
    "            \n",
    "            # Backward Propagation\n",
    "            #initialize new weights and biases\n",
    "            dW1 = np.zeros((self.input_layer, self.hidden_layer))\n",
    "            dW2 = np.zeros((self.hidden_layer, self.output_layer))\n",
    "            dB1 = np.zeros(self.hidden_layer)\n",
    "            dB2 = np.zeros(self.output_layer)\n",
    "            \n",
    "            #calculate bias from output layer to hidden layer\n",
    "            #bias b'2= (y-ytrue) * f'(y_)\n",
    "            for k in range(self.output_layer):\n",
    "                dB2[k] = (self.y[k] - y_true[k]) * self.sigmoid_der(y_[k])\n",
    "            \n",
    "            #calculate weight from output layer to hidden layer\n",
    "            #weight w'2= b'2 * h1\n",
    "            for j in range(self.hidden_layer):\n",
    "                for k in range(self.output_layer):\n",
    "                    dW2[j][k] =  dB2[k] * h1[j]\n",
    "\n",
    "            #calculate bias from hidden layer to input layer\n",
    "            #bias b'1= sum (b'2 * w'2 * f'(h1))\n",
    "            for j in range(self.hidden_layer):\n",
    "                dB1[j] = sum([dB2[k] * self.W2[j][k] * self.sigmoid_der(h1_[j]) for k in range(self.output_layer)])\n",
    "\n",
    "            #calculate weight from hidden layer to input layer\n",
    "            #weight w'1 = b'1 * input\n",
    "            for i in range(self.input_layer):\n",
    "                for j in range(self.hidden_layer):\n",
    "                    dW1[i][j] = dB1[j] * x[i]\n",
    "            \n",
    "        return self.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90683733])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of neurons for each layer\n",
    "input_layer = 2\n",
    "hidden_layer = 4\n",
    "output_layer = 1\n",
    "W1 = np.random.rand(input_layer, hidden_layer)\n",
    "B1 = np.random.rand(hidden_layer)\n",
    "W2 = np.random.rand(hidden_layer, output_layer)\n",
    "B2 = np.random.rand(output_layer)\n",
    "nunet = NN(input_layer,hidden_layer,output_layer,W1,W2,B1,B2)\n",
    "x_train = np.array([[0,1], [1,1], [1,0], [1,1]])\n",
    "y_train = np.array([[1], [1], [0], [0]])\n",
    "nunet.forwardAndBackwardPro(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. The random variable X and Y have the following joint probability density <br/>\n",
    "ğ‘“ğ‘‹ğ‘Œ(ğ‘¥, ğ‘¦) = ğ‘’âˆ’ğ‘¥âˆ’ğ‘¦ 0 < ğ‘¥ < âˆ,0 < ğ‘¦ < âˆ <br />\n",
    "            0 ğ‘’ğ‘™ğ‘ ğ‘’ğ‘¤â„ğ‘’ğ‘Ÿğ‘’\n",
    "What is ğ‘ƒ(ğ‘‹ < ğ‘Œ) ? <br/>\n",
    "\n",
    "sol. <br/>\n",
    "Both random variables are continuous as they have joint probability density function. <br />\n",
    "P(X<Y) = $$\\int_{0}^x\\int_{x}^y fXY(x,y)dxdy$$  <br/>\n",
    "$$\\int_{0}^x\\int_{x}^y 1/e^x  1/e^y dxdy$$  <br/>\n",
    "= 1/e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
